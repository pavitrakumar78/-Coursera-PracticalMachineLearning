{"name":"-coursera-practicalmachinelearning","tagline":"A course on practical machine learning using R","body":"#[Coursera]PracticalMachineLearning\r\n\r\n## Practical Machine Learning Assignment Writeup\r\n\r\nOur goal in this assignment is to build a predictive model to predict the correctness \r\nof a participant's excercise form using data from accelerometers on the belt, forearm, arm, and dumbell.\r\n\r\n##Data\r\n\r\nThe data for this assignment is provided by Groupware@LES  \r\nRead more on Human Activity Recognition: http://groupware.les.inf.puc-rio.br/har\r\n\r\nTraining data can be downloaded from the below link:\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\r\n\r\nTest data can be downloaded from the below link:\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\r\n\r\n##Analysis in R\r\n\r\nLibraries used for this assignment:\r\n\r\n```r\r\nlibrary(caret)\r\nlibrary(randomForest)\r\nlibrary(doParallel)\r\nlibrary(\"foreach\")\r\nlibrary(\"doSNOW\")\r\n```\r\n\r\nSetting seed for reproducibility\r\n\r\n```r\r\nset.seed(1234)\r\n```\r\n\r\nWe first load the csv files obtained from the links given in the data section and process them.\r\nThe below functions are used for loading and processing the data.\r\n\r\nThe ```ct``` function is used to get the count of NA values of a column and the columns above a threshold value are deleted\r\n(in this case it is 20 - this suits both the training data and the testing data so no need to make 2 seperate functions\r\nor modifiy the existing one to process test data)\r\n\r\n```r\r\nct <- function(x){\r\n\treturn(length(which(is.na(x))))\r\n}\r\n```\r\n\r\nThe given data as it is has a lot of redundant columns for our analysis and also contains a lot of NA values.\r\nSo, we use the ```process_data``` function to load the data and remove unwanted columns and NA values.\r\n\r\n```r\r\nprocess_data <- function(dir){\r\n\tprocessed_data <- read.csv(dir)\r\n\t\r\n\t#deleting unwanted columns\r\n\t#removing index,user_name,raw_time data fields,cvtd_time,new_window,num_window\r\n\r\n\tprocessed_data[1:7] = list(NULL)\r\n\r\n\t#cleaning data\r\n\t#removing cols. with >= 20 NA's\r\n\t\r\n\t# removing the last column in the names list best it needs no conversion\r\n\t# it is 'classe' column in training data and 'problem_id' in testing data (the last column in testing data is removed        #later)\r\n\t\r\n\tnames_list <- names(processed_data)[-length(names(processed_data))]\r\n\r\n\tfor(n in names_list){\r\n\t\tprocessed_data[[n]] <- as.numeric(as.character(processed_data[[n]]))\r\n\t\tif(ct(processed_data[[n]])>=20){\r\n\t\t\tprocessed_data[[n]] <- NULL\r\n\t\t}\r\n\t}\r\n\treturn(processed_data)\r\n}\r\n```\r\nBefore processing the dimensions of the training data are:\r\n\r\n```r\r\ntrain_data <- read.csv(\"pml-training.csv\")\r\ndim(train_data)\r\n\r\n#[1] 19622 160\r\n```\r\n\r\nAfter processing data:\r\n```r\r\ntrain_data <- process_data(\"pml-training.csv\")\r\ndim(train_data)\r\n\r\n#[1] 19622 52\r\n```\r\n\r\nWe have reduced the column count to 1/3rd of original data.\r\n\r\nNow, We apply the same function on the given training data and split it into testing(25%) and training data(75%)\r\nto check for our model acuracy.\r\n\r\n```r\r\ntrain_data <- process_data(\"pml-training.csv\")\r\ninTrain <- createDataPartition(y = train_data$classe,p = 0.75,list = F)\r\n\r\ntraining <- train_data[inTrain,]\r\ntesting <- train_data[-inTrain,]\r\n\r\ndim(testing)\r\n#[1] 4904   53\r\n\r\ndim(training)\r\n#[1] 14718    53\r\n```\r\n\r\nWe train using the random forest model.\r\n\r\n```r\r\nregisterDoSNOW(makeCluster(4, type=\"SOCK\"))\r\n\r\nmodelFit <- foreach(ntree=rep(150, 4), .combine = combine, .packages = \"randomForest\") %dopar% randomForest(training, training$classe, ntree=ntree, keep.forest=TRUE)\r\n```\r\nUsing the above trained model to predict the values of testing data.\r\n\r\n```r\r\npredicted_in_train <- predict(modelFit,newdata = testing)\r\nconfusionMatrix(predicted_in_train, testing$classe)\r\n```\r\n\r\n##Results:\r\n\r\n```r\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 1395    0    0    0    0\r\n         B    0  949    0    0    0\r\n         C    0    0  855    0    0\r\n         D    0    0    0  804    0\r\n         E    0    0    0    0  901\r\n\r\nOverall Statistics\r\n                                     \r\n               Accuracy : 1          \r\n                 95% CI : (0.9992, 1)\r\n    No Information Rate : 0.2845     \r\n    P-Value [Acc > NIR] : < 2.2e-16  \r\n                                     \r\n                  Kappa : 1          \r\n Mcnemar's Test P-Value : NA         \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            1.0000   1.0000   1.0000   1.0000   1.0000\r\nSpecificity            1.0000   1.0000   1.0000   1.0000   1.0000\r\nPos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\nNeg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\nPrevalence             0.2845   0.1935   0.1743   0.1639   0.1837\r\nDetection Rate         0.2845   0.1935   0.1743   0.1639   0.1837\r\nDetection Prevalence   0.2845   0.1935   0.1743   0.1639   0.1837\r\nBalanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000\r\n```\r\n\r\nWe can conclude from the above confusion matrix that out model has obtained 100% accuracy on classifying 25% of given test data and since it is also efficient - takes about ~2 minutes to train ~20k rows (full training data) and also gives high accuracy, further model tuning seems unnessary.\r\n\r\n##Predicting given test data for submission\r\n\r\nWe use the same ```data_process``` fuction to process the testing data.\r\n\r\n```r\r\ntrain_data <- process_data(\"pml-training.csv\")\r\ntest_data <- process_data(\"pml-testing.csv\")\r\ny <- train_data$classe\r\ntrain_data$classe <- NULL\r\n\r\nregisterDoSNOW(makeCluster(4, type=\"SOCK\"))\r\nmodelFit <- foreach(ntree=rep(200, 4), .combine = combine, .packages = \"randomForest\") %dopar% randomForest(train_data, y, ntree=ntree, keep.forest=TRUE)\r\n\r\ntest_data$problem_id <- NULL\r\n```\r\n\r\nAfter processing the testing and training data, we predict the values using our trained model and write them into text files.\r\n\r\n```r\r\npredicted_in_test <- predict(modelFit,newdata = test_data)\r\n\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\n\r\npml_write_files(predicted_in_test)\r\n```\r\n\r\n##Conclusion\r\n\r\nThe predicted data scored 100% on the course project submission, so we can conclude that using random forest as ouu training mode and cleaning and processing the data carefully we have a highly accuracte prediction model.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}